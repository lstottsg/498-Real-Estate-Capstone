
#############################################
# PREDICT 498 #
# Real Estate Group #
#############################################


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# LOAD PACKAGES AND DATA
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# load libraries but install them first
list.of.packages <- c(
  "car" 
  ,"missForest" #fill in missing values
  ,"Hmisc"
  ,"corrplot" #correlation plot
  ,"lattice" #quantile plots
  ,"leaps" # Crossfold validation for best subsets
  ,"MASS" # for qda and lda
  ,"gam" # for gam
  ,"class" # for knn
  ,"randomForest" #RF
  ,"xgboost" #gradient boosting
  ,"Matrix" #sparse.model.matrix
  ,"glmnet"
  ,"leaps"
  ,"reshape"
  ,"tibble"
  ,"tidyverse" # tidyverse is a wrapper that contains dplyr, ggplot, tidyr, readr and a bunch of other great Hadley stuff
  ,"survival"
  ,"OIsurv" # Automatically loads KMsurv - bmt example
  ,"ranger"
  ,"doBy"
)
new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,"Package"])]
if(length(new.packages)) install.packages(new.packages)
lapply(list.of.packages,library, character.only=T)

#set working directory
setwd("C:/Users/lstottsg/Desktop/Laura School/Summer 2017/498/Data")

# Load Data
rm(list=ls())

#import files
mls <- read.csv('mls_Data_20120315_20170315.csv')
bus <- read.csv('Census_Businesses.csv')
demo <- read.csv('Census_Demographics1.csv')
zmarket <- read.csv('Zillow_Market_Metrics.csv')
zratio <- read.csv('Zillow_price_to_rent_ratio.csv')
zsf <- read.csv('zillow_sf_price.csv')

#combine census files
census1 <- merge(x = demo, y = bus, by="Area_Name")

zmarket2 <- zmarket[,c(2,8)]

census2 <- merge(x = census1, y = zmarket2, by.x="Area_Name", by.y="RegionName")

#combine mls data with census data
d1 <- merge(x = mls, y = census2, by.x="Zip", by.y = "Area_Name")


##########################################
#        Exploratory Data Analyis        #
##########################################

#view data dimensions
dim(mls)
dim(bus)
dim(demo)
dim(zmarket)
dim(zratio)
dim(zsf)

#look at summaries
summary(d1)

#look at NA values
sapply(d1, function(x) sum(is.na(x)))

str(d1)
glimpse(d1)

#gather variable names in table format
d1.names <- as_data_frame(names(d1))

#write this out for a summary table
#write.table(d1.names, file = "d1_names.txt", sep = ",", quote = FALSE, row.names = F)

#create eda dataset
d1.eda <- d1


attach(d1.eda)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# IMPUTE NA VALUES
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#Find mean values to impute for continuous numeric missing values
#create new variable for possible use later as well
M_TotalFnishedSF <- mean(d1.eda$TotalFinishedSF, na.rm=TRUE)
M_AboveGradeFinishedSF <- mean(d1.eda$AboveGradeFinishedSF, na.rm=TRUE)
M_BelowGradeFinishedSF <- mean(d1.eda$BelowGradeFinishedSF, na.rm=TRUE)
M_BelowGradeSF <- mean(d1.eda$BelowGradeSF, na.rm=TRUE)
M_Acreage <- mean(d1.eda$Acreage, na.rm=TRUE)
M_PricePerSqFt <- mean(d1.eda$PricePerSqFt, na.rm=TRUE)

#Find median values to impute for non-continuous numeric missing values
#create new variable for possible use later as well
Md_Beds <- median(d1.eda$Beds, na.rm=TRUE)
Md_FullBaths <- median(d1.eda$FullBaths, na.rm=TRUE)
Md_GarageSize.Cars <- median(d1.eda$GarageSize.Cars, na.rm=TRUE)
Md_LavsHalfBaths <- median(d1.eda$LavsHalfBaths, na.rm=TRUE)
Md_NoOfUnits <- median(d1.eda$NoOfUnits, na.rm=TRUE)

#Impute mean for continuous numeric missing values
d1.eda$TotalFinishedSF<-ifelse(is.na(d1.eda$TotalFinishedSF)==TRUE,M_TotalFnishedSF,d1.eda$TotalFinishedSF)
d1.eda$AboveGradeFinishedSF<-ifelse(is.na(d1.eda$AboveGradeFinishedSF)==TRUE,M_AboveGradeFinishedSF,d1.eda$AboveGradeFinishedSF)
d1.eda$BelowGradeFinishedSF<-ifelse(is.na(d1.eda$BelowGradeFinishedSF)==TRUE,M_BelowGradeFinishedSF,d1.eda$BelowGradeFinishedSF)
d1.eda$BelowGradeSF<-ifelse(is.na(d1.eda$BelowGradeSF)==TRUE,M_BelowGradeSF,d1.eda$BelowGradeSF)
d1.eda$Acreage<-ifelse(is.na(d1.eda$Acreage)==TRUE,M_Acreage,d1.eda$Acreage)
d1.eda$PricePerSqFt<-ifelse(is.na(d1.eda$PricePerSqFt)==TRUE,M_PricePerSqFt,d1.eda$PricePerSqFt)

#Impute median for non-continuous numeric missing values
d1.eda$Beds<-ifelse(is.na(d1.eda$Beds)==TRUE,Md_Beds,d1.eda$Beds)
d1.eda$FullBaths<-ifelse(is.na(d1.eda$FullBaths)==TRUE,Md_FullBaths,d1.eda$FullBaths)
d1.eda$GarageSize.Cars<-ifelse(is.na(d1.eda$GarageSize.Cars)==TRUE,Md_GarageSize.Cars,d1.eda$GarageSize.Cars)
d1.eda$LavsHalfBaths<-ifelse(is.na(d1.eda$LavsHalfBaths)==TRUE,Md_LavsHalfBaths,d1.eda$LavsHalfBaths)
d1.eda$NoOfUnits<-ifelse(is.na(d1.eda$NoOfUnits)==TRUE,Md_NoOfUnits,d1.eda$NoOfUnits)

# populate Waterfront YN with Y if there is a WaterAccessType not in "no" or "none", else N
d1.eda$WaterAccessYN <- ifelse(is.na(d1.eda$WaterAccessYN)
                               , ifelse(d1.eda$WaterAccessType=='No'
                                        , ifelse(d1.eda$WaterAccessType=='None'
                                                 , 'N'
                                                 , 'Y')
                                        , 'Y')
                               , d1.eda$WaterAccessYN)

#convert factor variables with fewer than 53 categories - can't use these in missForest and most shouldn't be factor anyway
d1.eda$Zip<-as.numeric(d1.eda$Zip)
d1.eda$ClosedDate<-as.Date(d1.eda$ClosedDate)
d1.eda$PendingDate<-as.Date(d1.eda$PendingDate)
d1.eda$ListDate<-as.Date(d1.eda$ListDate)
d1.eda$LastStatusChangeDate<-as.Date.factor(d1.eda$LastStatusChangeDate)
d1.eda$Address<-as.character(d1.eda$Address)
d1.eda$AddressNumber<-as.numeric(d1.eda$AddressNumber)
d1.eda$Street<-as.character(d1.eda$Street)
d1.eda$City<-as.character(d1.eda$City)
d1.eda$Township<-as.character(d1.eda$Township)
d1.eda$County<-as.character(d1.eda$County)
d1.eda$AreaName<-as.character(d1.eda$AreaName)
d1.eda$WaterAccessType<-as.character(d1.eda$WaterAccessType)
d1.eda$WaterFrontage<-as.character(d1.eda$WaterFrontage)
d1.eda$WaterfrontDescription<-as.character(d1.eda$WaterfrontDescription)
d1.eda$LakeType<-as.character(d1.eda$LakeType)
d1.eda$BodyofWaterName<-as.character(d1.eda$BodyofWaterName)
d1.eda$LotSize<-as.character(d1.eda$LotSize)
d1.eda$Style<-as.character(d1.eda$Style)
d1.eda$Sewer<-as.character(d1.eda$Sewer)
d1.eda$WaterPublicWell<-as.character(d1.eda$WaterPublicWell)
d1.eda$BsmtType<-as.character(d1.eda$BsmtType)
d1.eda$GarageType<-as.character(d1.eda$GarageType)
d1.eda$HeatingType<-as.character(d1.eda$HeatingType)
d1.eda$CoolingCentralAC<-as.character(d1.eda$CoolingCentralAC)
d1.eda$SchoolDistrict<-as.character(d1.eda$SchoolDistrict)
d1.eda$ElementarySchool<-as.character(d1.eda$ElementarySchool)
d1.eda$MiddleSchool<-as.character(d1.eda$MiddleSchool)
d1.eda$HighSchool<-as.character(d1.eda$HighSchool)
d1.eda$ListingAgent<-as.character(d1.eda$ListingAgent)
d1.eda$ListingAgentID<-as.numeric(d1.eda$ListingAgentID)
d1.eda$ListingBroker<-as.character(d1.eda$ListingBroker)
d1.eda$ListingBrokerID<-as.numeric(d1.eda$ListingBrokerID)
d1.eda$SellingAgentName<-as.character(d1.eda$SellingAgentName)
d1.eda$SellingAgentID<-as.numeric(d1.eda$SellingAgentID)
d1.eda$SellingBroker<-as.character(d1.eda$SellingBroker)
d1.eda$SellingBrokerID<-as.numeric(d1.eda$SellingBrokerID)
d1.eda$Zoning<-as.character(d1.eda$Zoning)


#Subset by type - missing 4 are dates
d1.num <- d1.eda[,sapply(d1.eda,is.numeric)]
d1.fac <- d1.eda[,sapply(d1.eda,is.factor)]
d1.char <- d1.eda[,sapply(d1.eda,is.character)]



#impute other missing values using missForest
#d1.num.imp <- missForest(d1.num)

#d1.imp$OOBerror
#d1.imp$OOBerror


# populate Seller Concession Values with ??? (Not sure best way)


summary(d1.eda)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# REMOVE NA OBSERVATIONS FOR IMPORTANT PREDICTORS
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

d1.eda<-d1.eda[(is.na(d1.eda$OriginalListPrice)=TRUE),]
d1.eda<-d1.eda[(is.na(d1.eda$PendingDate)=TRUE),]


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# CREATE PREDICTOR VARIABLES
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#create DaysListToPending variable for prediction
d1.eda$DaysListToPending <- as.numeric(as.Date.factor(d1.eda$PendingDate) - as.Date.factor(d1.eda$ListDate))

#create PendingIndicator variable for prediction
#this will include status of 
#1 for sold, pending, leased, off market
#0 for active, canceled, contingency, deleted, expired, not found in, prospect, withdrawn
d1.eda$PendingIndicator <- d1.eda$ListingStatus

original.pending.levels <- unique(d1.eda$ListingStatus)

levels(d1.eda$PendingIndicator) <- c(0,0,0,0,0,1,0,0,1,1,0,1,0)              
d1.eda$PendingIndicator <- as.numeric(as.character(d1.eda$PendingIndicator))  

new.pending.levels <- unique(d1.eda$PendingIndicator)

#we have a data set heavily weighted with pending and sold properties
mean(d1.eda$PendingIndicator, na.rm = T)
#0.9629795

#verify new variables look correct
head(d1.eda)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# REMOVE INELIGIBLE / NON-PREDICTIVE VARIABLES
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# variables to exclude from training and test data
exclude_vars <- 
  c("X_BATCH_ID_"
    ,"MLS_Number"
    ,"ClosedDate"
    ,"PendingDate"
    ,"ClosedSoldPrice"
    ,"DaysOnMarket"
    ,"WebPropertyType.1")

d1.eda <- d1.eda[!names(d1.eda)%in%exclude_vars]


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# REVIEW SUMMARY DATA AGAIN
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


#look at summary again after imputations and removals
summary(d1.eda)
glimpse(d1.eda)

#Hmisc package to get further info like high/low, distinct, mean, frequency
library(Hmisc)
describe(d1.eda)

#load pych package to see skew, kurtosis, se for numeric values
library(psych)
describe(d1.eda$DaysListToPending)



#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# SUBSET
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#use package doBy to find summary stats by group
summaryBy(YearBuilt ~ SchoolDistrict, data = d1.eda, 
          FUN = list(mean, max, min, median, sd))

#create data frame to reference for plotting
yearBuilt_by_schoolDistrict1 <- as_data_frame(summaryBy(YearBuilt ~ SchoolDistrict, data = d1.eda, 
                                                        FUN = list(mean, max, min, median, sd)
)
)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# CORRELATIONS
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#Pearson correlations for numeric values
rcorr(as.matrix(d1.num), type="pearson")

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# PLOT
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#scatterplot matrix of numeric values
scatterplotMatrix(mls.num, smoother=FALSE)


##########################################
#        TRANSFORMATIONS                 #
##########################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# HANDLE OUTLIERS
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

#look at percentages for OriginalListPrice to decide how best to trim outliers
min(d1.eda$`Year Built`)
max(d1.eda$`Year Built`)
quantile(d1.eda$`Year Built`, c(.01, .05, .1, .15, .85, .95, .98, .99)) 

d1.eda<-d1.eda[!(d1.eda$OriginalListPrice>=3000000),]

#look at percentages for ListPrice to decide how best to trim outliers
min(d1.eda$`Year Built`)
max(d1.eda$`Year Built`)
quantile(d1.eda$`Year Built`, c(.01, .05, .1, .15, .85, .95, .98, .99)) 

d1.eda<-d1.eda[!(d1.eda$ListPrice>=3000000),]


#look at percentages for DaysonMkt to decide how best to trim outliers
min(d1.eda$`Year Built`)
max(d1.eda$`Year Built`)
quantile(d1.eda$`Year Built`, c(.01, .05, .1, .15, .85, .95, .98, .99)) 

d1.eda<-d1.eda[!(d1.eda$DaysonMkt<0),]
d1.eda<-d1.eda[!(d1.eda$DaysonMkt>=365*2),]






#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# DUMMY CODING / NEW VARIABLES
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

d1.eda$WaterAccess_ID <- ifelse(d1.eda$WaterAccessYN=="Y",1,ifelse(d1.eda$WaterAccessYN=="Yes",1,ifelse(d1.eda$WaterAccessYN=="Deeded Access",1,0)))
d1.eda$On_Lake_Michigan <- ifelse(d1.eda$BodyofWaterName=="Lake Michigan",1,0)
d1.eda$On_Pond <- ifelse(d1.eda$BodyofWaterName=="Pond",1,0)
d1.eda$Condo_ID <- ifelse(d1.eda$PropertyType=="Condo",1,0)
d1.eda$Multi_Fam_ID <- ifelse(d1.eda$PropertyType=="Multi Family",1,0)
d1.eda$Well_Water_ID <- ifelse(d1.eda$WaterPublicWell=="Well",1,0)
d1.eda$Walk_Out_Basement_ID <-ifelse(d1.eda$BsmtType=="Walk Out",1,0)
d1.eda$Finished_Basement_ID <- ifelse(d1.eda$BsmtType=="Finished",1,ifelse(d1.eda$BsmtType=="Partially Finished",1,0))
d1.eda$Garage_ID <-ifelse(d1.eda$GarageYN=="N",0,1)
d1.eda$Concession_ID <-ifelse(d1.eda$ConcessionYN=="Yes",1,ifelse(d1.eda$ConcessionYN=="Financing",1,ifelse(d1.eda$ConcessionYN=="Other",1,0)))
d1.eda$Foreclosure_ID <- ifelse(d1.eda$ForeclosureYN=="f",1,0)
d1.eda$Rental_ID <-ifelse(d1.eda$LeaseRentYN=="Y",1,0)
d1.eda$Sale_ID <-ifelse(d1.eda$SaleLease=="Sale",1,0)
d1.eda$GP_Schools <- ifelse(d1.eda$SchoolDistrict=="Grosse Pointe",1,0)
d1.eda$Other_Good_Schools <-ifelse(d1.eda$SchoolDistrict=="Bloomfield Hills",1,ifelse(d1.eda$SchoolDistrict=="West Bloomfield",1,ifelse(d1.eda$SchoolDistrict=="Birmingham",1,ifelse(d1.eda$SchoolDistrict=="Rochester",1,0))))
d1.eda$Beds <- ifelse(d1.eda$Beds>10, 10,d1.eda$Beds)
d1.eda$YearBuilt<-ifelse(d1.eda$YearBuilt>=2018,1995,d1.eda$YearBuilt)
d1.eda$TotalFinishedSF<-ifelse(d1.eda$TotalFinishedSF>10000,10000,d1.eda$TotalFinishedSF)
d1.eda$AboveGradeFinishedSF<-ifelse(d1.eda$AboveGradeFinishedSF>10000,10000,d1.eda$AboveGradeFinishedSF)
d1.eda$BelowGradeFinishedSF<-ifelse(d1.eda$BelowGradeFinishedSF>10000,10000,d1.eda$BelowGradeFinishedSF)
d1.eda$BelowGradeSF<-ifelse(d1.eda$BelowGradeSF>10000,10000,d1.eda$BelowGradeSF)
d1.eda$Acreage<-ifelse(d1.eda$Acreage>1000,1000,d1.eda$Acreage)
d1.eda$FullBaths<-ifelse(d1.eda$FullBaths>10,10,d1.eda$FullBaths)
d1.eda$GarageSize.Cars<-ifelse(d1.eda$GarageSize.Cars>10,10,d1.eda$GarageSize.Cars)
d1.eda$LavsHalfBaths<-ifelse(d1.eda$LavsHalfBaths>10,10,d1.eda$LavsHalfBaths)
d1.eda$PricePerSqFt<-ifelse(d1.eda$PricePerSqFt>1000,1000,d1.eda$PricePerSqFt)

#Remove the rest of the NA observations
d1 <- na.omit(d1)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# BINNING
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# create bins of 10
bin_10 <- function(x) ntile(x,10)


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# MATHEMATICAL
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# square transform
var_square <- function(x) x^2

# log transform
var_log <- function(x) log(abs(x)+1)*sign(x)

# sqrt transform
var_log <- function(x) log(abs(x)+1)*sign(x)

# cubrt transform
var_log <- function(x) log(abs(x)+1)*sign(x)

# reciprocal transform
var_log <- function(x) log(abs(x)+1)*sign(x)



##########################################
#        PARTITION DATA                  #
##########################################

n = nrow(d1.eda)
trainIndex = sample(1:n, size=round(0.6*n),replace=FALSE)
train=d1.eda[trainIndex,]
test=d1.eda[-trainIndex,]

require(rpart)
require(rpart.plot)
require(tree)
require(rattle)
require(caTools)
require(ROCR)
require(ResourceSelection)

library(corrgram)
library(MASS)
library(randomForest)
library(inTrees)
library(pROC)
library(caret)
library(dplyr)


summary(d1.eda)


##########################################
# REGRESSION & RANDOM FOREST ANALYSIS    #
##########################################


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Linear Regression
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

lm1 <-lm(DaysonMkt ~ TotalFinishedSF + FullBaths + OriginalListPrice + YearBuilt +WaterAccess_ID + PricePerSqFt +
           On_Lake_Michigan + On_Pond + Condo_ID + Multi_Fam_ID + Walk_Out_Basement_ID + Finished_Basement_ID +
           Garage_ID + Concession_ID + Foreclosure_ID + Rental_ID + GP_Schools + Other_Good_Schools , data=train) 

summary(lm1)

pred.valid.lm1 <- predict(lm1, newdata = test) # validation predictions





lm2 <-lm(DaysonMkt ~ TotalFinishedSF + FullBaths + OriginalListPrice + YearBuilt +WaterAccess_ID + PricePerSqFt +
           On_Lake_Michigan + On_Pond + Condo_ID + Multi_Fam_ID + Walk_Out_Basement_ID + Finished_Basement_ID +
           Garage_ID + Concession_ID + Foreclosure_ID + Rental_ID + GP_Schools + Other_Good_Schools +
           pop_density + renter + hh_ave_size + vacant + pct_pop_growth, data=train) 

summary(lm2)

pred.valid.lm2 <- predict(lm2, newdata = test) # validation predictions


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Random Forest 1
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


rf1 <-randomForest(DaysonMkt ~ TotalFinishedSF + FullBaths + OriginalListPrice + YearBuilt +WaterAccess_ID + PricePerSqFt +
                     On_Lake_Michigan + On_Pond + Condo_ID + Multi_Fam_ID + Walk_Out_Basement_ID + Finished_Basement_ID +
                     Garage_ID + Concession_ID + Foreclosure_ID + Rental_ID + GP_Schools + Other_Good_Schools , data=train, importance=TRUE, ntree=5) 

summary(rf1)
pringt(rf1)
plot(rf1)
importance(rf1)
varImpPlot(rf1)
par(mfrow=c(1,1))
#get the prediction probabilities##
rf1p  <- predict(rf1, newdata=test1,type="response")
head(rf1p)
hist(rf1p)
sort(rf1p)
rf1.pred = prediction(rf1p, test$DaysonMkt)
rf1.perf = performance(rf1.pred,"tpr","fpr")
plot(rf1.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

auc(test$DaysonMkt,rf1p)

#Make a confusion matrix and compute accuracy
##confusion.matrix <- table(rf1p, subdat3$RESPONSE16)
##confusion.matrix

rf1pround <- round(rf1p,0)
xtabs(~DaysonMkt + rf1pround, data = test)

rf1pdf <- as.data.frame(rf1p)
data_alldf <- cbind(d1,rf1pdf)
str(data_alldf)
head(data_alldf)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Random Forest 2
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

rf1 <-randomForest(DaysonMkt ~ TotalFinishedSF + FullBaths + OriginalListPrice + YearBuilt +WaterAccess_ID + PricePerSqFt +
                     On_Lake_Michigan + On_Pond + Condo_ID + Multi_Fam_ID + Walk_Out_Basement_ID + Finished_Basement_ID +
                     Garage_ID + Concession_ID + Foreclosure_ID + Rental_ID + GP_Schools + Other_Good_Schools +
                     pop_density + renter + hh_ave_size + vacant + pct_pop_growth, data=train, importance=TRUE, ntree=5) 

summary(rf1)
pringt(rf1)
plot(rf1)
importance(rf1)
varImpPlot(rf1)






par(mfrow=c(1,1))
#get the prediction probabilities##
rf1p  <- predict(rf1, newdata=test1,type="response")
head(rf1p)
hist(rf1p)
sort(rf1p)
rf1.pred = prediction(rf1p, test$DaysonMkt)
rf1.perf = performance(rf1.pred,"tpr","fpr")
plot(rf1.perf,main="ROC Curve for Random Forest",col=2,lwd=2)
abline(a=0,b=1,lwd=2,lty=2,col="gray")

auc(test$DaysonMkt,rf1p)

#Make a confusion matrix and compute accuracy
##confusion.matrix <- table(rf1p, subdat3$RESPONSE16)
##confusion.matrix

rf1pround <- round(rf1p,0)
xtabs(~DaysonMkt + rf1pround, data = test)

rf1pdf <- as.data.frame(rf1p)
data_alldf <- cbind(d1,rf1pdf)
str(data_alldf)
head(data_alldf)



##########################################
#        SURVIVAL ANALYSIS               #
##########################################

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# Kaplan Meier Analysis
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Kaplan Meier Survival Curve
y_mls <- Surv(mls.model$DaysListToPending, mls.model$PendingIndicator)
y_mls

fit1_mls <- survfit(y_mls ~ 1)
summary(fit1_mls)

#Kaplan Meier Plot with Confidence Bands
cb <- confBands(y_mls, type = "hall")
plot(fit1_mls,
     main = 'Kaplan Meier Plot with Confidence Bands')
lines(cb, col = "red",lty = 3)
legend(1000, 0.99, legend = c('K-M Survival Estimate',
                              'Pointwise Intervals', 'Hall-Werner Conf Bands'), lty = 1:3)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Cox Proportional Hazards Model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# Fit Cox Model
form <- formula(y_mls ~ mls.model$`Original List Price`) #Add predictors here...

cox_bmt <- coxph(form,data = mls.model)
summary(cox_mls)

cox_fit_mls <- survfit(cox_mls)
plot(cox_fit_mls)

#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
#Random Forests Model
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

# ranger model
r_fit_mls <- ranger(form,
                    data = mls,
                    importance = "permutation",
                    seed = 1234)

# Average the survival models
pending_times <- r_fit_mls$unique.death.times
surv_prob <- data.frame(r_fit_mls$survival)
avg_prob <- sapply(surv_prob,mean)

# Plot the survival models for each patient
plot(r_fit_mls$unique.death.times,r_fit_mls$survival[1,], type = "l", 
     ylim = c(0,1),
     col = "red",
     xlab = "pending sale",
     ylab = "survival",
     main = "Pending Sale Curves")

for(n in c(2:137)){
  lines(r_fit_mls$unique.death.times, r_fit_mls$survival[n,], type = "l", col = "red")
}
lines(death_times, avg_prob, lwd = 2)
legend(100, 0.2, legend = c('Averages - black'))


#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# FINAL RESULTS
#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~






